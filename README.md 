# Duck Dive üèÑ‚Äç‚ôÇÔ∏è

**The clean and accurate way for surfers to get surf data**

## Project Overview

Duck Dive is a minimalist surf forecasting app that provides clean, accurate surf conditions for multiple spots. The app consists of a single HTML frontend and FastAPI backend that pulls real-time surf data using the surfpy library.

## Tech Stack

- **Frontend**: Single HTML file with Tailwind CSS (mobile-friendly)
- **Backend**: FastAPI (Python)
- **Database**: Supabase
- **Data Source**: surfpy library (abenstirling fork) + NOAA data
- **Deployment**: Single Python file + HTML file

## Features

### Core Functionality
- **Random spot redirect**: `/` redirects to a random surf spot
- **Spot pages**: `/{spot}` shows current conditions for that spot
- **Mobile-responsive**: Clean 2x2 grid showing key surf metrics
- **Auto-refresh**: Page refreshes every 5 minutes
- **Spot navigation**: Navbar dropdown to switch between spots

### API Endpoints
- `GET /api/get_report?spot=tamarack` - Get latest conditions for a spot
- `POST /api/refresh_reports` - Refresh all spot data (for cron jobs)
- `POST /api/new_spot_request` - Submit new spot requests

### Data Displayed
- Wave height (ft)
- Tide height (ft) 
- Wind speed (mph)
- Water temperature (¬∞F)
- Stream/webcam link (if available)

## Database Schema

### Main Reports Table: `surf_reports`
```sql
CREATE TABLE surf_reports (
    id SERIAL PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL,
    spot TEXT NOT NULL,
    wave_height_ft REAL,
    tide_height_ft REAL,
    water_temp_f REAL,
    wind_mph REAL,
    stream_link TEXT
);
```

### Spot Requests Table: `spot_requests`
```sql
CREATE TABLE spot_requests (
    id SERIAL PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL,
    email TEXT NOT NULL,
    spot_name TEXT NOT NULL,
    implemented BOOLEAN DEFAULT FALSE
);
```

## Installation & Setup

### 1. Clone and Install Dependencies

```bash
git clone <your-repo>
cd duck-dive
pip install -r requirements.txt
```

### requirements.txt
```txt
fastapi==0.104.1
uvicorn==0.24.0
supabase==2.0.0
matplotlib==3.8.0
numpy==1.25.0
git+https://github.com/abenstirling/surfpy.git
```

### 2. Environment Variables

Create a `.env` file:
```env
SUPABASE_URL=your_supabase_project_url
SUPABASE_KEY=your_supabase_anon_key
```

### 3. Supabase Setup

1. Create a new Supabase project
2. Run the SQL schema above in the SQL editor
3. Get your project URL and anon key from Settings > API

### 4. Configure Surf Spots

Edit the `SURF_SPOTS` dictionary in `main.py` with your desired locations:

```python
SURF_SPOTS = {
    "tamarack": {"lat": 33.0742, "lon": -117.3095, "depth": 25.0, "angle": 225.0},
    "blacks": {"lat": 32.8899, "lon": -117.2531, "depth": 30.0, "angle": 270.0},
    "scripps": {"lat": 32.8668, "lon": -117.2533, "depth": 20.0, "angle": 260.0},
    "windansea": {"lat": 32.8311, "lon": -117.2789, "depth": 15.0, "angle": 240.0}
}
```

**How to find depth and angle:**
- **Depth**: Use NOAA bathymetric charts or Google Earth ocean layer (typical: 15-30ft)
- **Angle**: Shore orientation in compass degrees (N=0¬∞, E=90¬∞, S=180¬∞, W=270¬∞)

## File Structure

```
duck-dive/
‚îú‚îÄ‚îÄ main.py              # FastAPI backend
‚îú‚îÄ‚îÄ data_fetcher.py      # Surf data fetching logic
‚îú‚îÄ‚îÄ requirements.txt     # Python dependencies
‚îú‚îÄ‚îÄ .env                # Environment variables
‚îî‚îÄ‚îÄ README.md           # This file
```

## Running the Application

### Development
```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Production
```bash
uvicorn main:app --host 0.0.0.0 --port 8000
```

Visit `http://localhost:8000` to see the app.

## Data Updates

### Automated Updates (Recommended)
Set up a cron job to refresh data every 30 minutes:

```bash
# Add to crontab (crontab -e)
*/30 * * * * curl -X POST https://yourdomain.com/api/refresh_reports
```

### Manual Updates
```bash
curl -X POST http://localhost:8000/api/refresh_reports
```

## Key Implementation Notes

### surfpy Integration
- Uses the abenstirling/surfpy fork for wave forecasting
- Fetches 24-hour forecasts from NOAA GFS wave models
- Processes wave heights, wind data, and breaking wave calculations
- Requires spot coordinates (lat/lon) and bathymetry info (depth/angle)

### Data Processing Flow
1. `data_fetcher.py` calls surfpy with spot coordinates
2. surfpy pulls NOAA wave and weather data
3. Wave breaking calculations performed using bathymetry
4. Current conditions extracted and saved to Supabase
5. Frontend displays latest data with auto-refresh

### Error Handling
- Graceful degradation when APIs are unavailable
- 404 pages for non-existent spots
- Data validation and error logging
- Fallback to "Loading..." when no data available

## API Response Examples

### GET /api/get_report?spot=tamarack
```json
{
    "timestamp": "2025-01-25T10:30:00Z",
    "spot": "tamarack",
    "wave_height_ft": 3.2,
    "tide_height_ft": 2.1,
    "water_temp_f": 63,
    "wind_mph": 8,
    "stream_link": null
}
```

### POST /api/refresh_reports
```json
{
    "status": "completed",
    "updated_spots": ["tamarack", "blacks", "scripps"],
    "errors": ["Error updating windansea: API timeout"],
    "timestamp": "2025-01-25T10:35:00Z"
}
```

## Frontend Features

- **Responsive design**: Works on mobile and desktop
- **Clean UI**: Minimal, focused on essential data
- **Auto-refresh**: Reloads every 5 minutes
- **Spot switching**: Dropdown navigation in header
- **Loading states**: Shows "Loading..." when no data
- **404 handling**: Clean error page for invalid spots

## Deployment

### Environment Setup
1. Set up production server (VPS, cloud instance, etc.)
2. Install Python 3.9+ and pip
3. Clone repository and install dependencies
4. Set environment variables
5. Set up reverse proxy (nginx recommended)
6. Configure domain and SSL

### Process Management
Use a process manager like `supervisor` or `systemd`:

```ini
# /etc/systemd/system/duck-dive.service
[Unit]
Description=Duck Dive Surf App
After=network.target

[Service]
User=www-data
WorkingDirectory=/path/to/duck-dive
Environment=PATH=/path/to/duck-dive/venv/bin
ExecStart=/path/to/duck-dive/venv/bin/uvicorn main:app --host 0.0.0.0 --port 8000
Restart=always

[Install]
WantedBy=multi-user.target
```

## Troubleshooting

### Common Issues
- **No data showing**: Check Supabase connection and run refresh_reports
- **surfpy errors**: Verify coordinates are correct and NOAA services are accessible
- **Slow loading**: Check internet connection and API response times
- **Database errors**: Verify Supabase credentials and table schema

### Debugging
- Check FastAPI logs for errors
- Test API endpoints individually
- Verify environment variables are set
- Check Supabase dashboard for data

## Future Enhancements

- Add tide charts and forecasting
- Implement user favorites
- Add push notifications
- Include surf cams/photos
- Expand to more regions
- Add historical data and trends

## Support

For issues related to:
- **surfpy**: Check the [abenstirling/surfpy](https://github.com/abenstirling/surfpy) repository
- **FastAPI**: See [FastAPI documentation](https://fastapi.tiangolo.com/)
- **Supabase**: Check [Supabase docs](https://supabase.com/docs)

---

**Made for surfers, by surfers** ü§ô